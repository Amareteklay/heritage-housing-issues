{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0aStgWSO0E0E"
      },
      "source": [
        "# Data Cleaning Notebook"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1eLEkw5O0ECa"
      },
      "source": [
        "## Objectives\n",
        "\n",
        "* Assess and handle missing values \n",
        "* Clean data\n",
        "\n",
        "## Inputs\n",
        "\n",
        "* outputs/datasets/collection/HousePrices.csv\n",
        "\n",
        "## Outputs\n",
        "\n",
        "* Cleaned data in outputs/datasets/cleaned\n",
        "* Data cleaning pipline\n",
        "\n",
        "## Conclusions\n",
        "\n",
        "* Drop ['EnclosedPorch', 'WoodDeckSF'] because each variable has more than 80% missing values.\n",
        "* Nine out of 24 variables (75% of the columns) have missing values.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9uWZXH9LwoQg"
      },
      "source": [
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cqP-UeN-z3i2"
      },
      "source": [
        "# Change working directory"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aOGIGS-uz3i2"
      },
      "source": [
        "We need to change the working directory from its current folder to its parent folder\n",
        "* We access the current directory with os.getcwd()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wZfF_j-Bz3i4",
        "outputId": "66943449-1436-4c3d-85c7-b85f9f78349b"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "current_dir = os.getcwd()\n",
        "current_dir"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9MWW8E7lz3i7"
      },
      "source": [
        "We want to make the parent of the current directory the new current directory\n",
        "* os.path.dirname() gets the parent directory\n",
        "* os.chdir() defines the new current directory"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TwHsQRWjz3i9",
        "outputId": "86849db3-cd2f-4cc5-ebb8-2d0caafa1a2c"
      },
      "outputs": [],
      "source": [
        "os.chdir(os.path.dirname(current_dir))\n",
        "print(\"You set a new current directory\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "M_xPk_Ijz3i-"
      },
      "source": [
        "Confirm the new current directory"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vz3S-_kjz3jA",
        "outputId": "00b79ae4-75d0-4a96-d193-ac9ef9847ea2"
      },
      "outputs": [],
      "source": [
        "current_dir = os.getcwd()\n",
        "current_dir"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-mavJ8DibrcQ"
      },
      "source": [
        "# Load Data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "df = pd.read_csv(\"outputs/datasets/collection/HousePrices.csv\")\n",
        "df.head(5)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Data Exploration"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "In the quick display of the first five rows of our dataframe object above, we already see four columns and several cells with missing values. Here we dig deeper and get all variables that have missing values.\n",
        "\n",
        " * Nine out of 24 variables (which is 75% of the columns) have missing values. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "vars_with_missing_data = df.columns[df.isna().sum() > 0].to_list()\n",
        "vars_with_missing_data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "len(vars_with_missing_data)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "df[vars_with_missing_data].info()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Profile Report\n",
        "\n",
        "* We generate pandas profiling report for each variable with missing values.\n",
        "  * We import pandas_profiling library and generate a profile report.\n",
        "  * Some variable have many zeros."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Code from walkthrough project 02, data cleaning notebook\n",
        "from pandas_profiling import ProfileReport\n",
        "if vars_with_missing_data:\n",
        "    profile = ProfileReport(df=df[vars_with_missing_data], minimal=True)\n",
        "    profile.to_notebook_iframe()\n",
        "else:\n",
        "    print(\"There are no variables with missing data\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZY3l0-AxO93d"
      },
      "source": [
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Data Cleaning"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Assessing Missing Data Levels"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "* Custom function to display missing data levels in a dataframe, it shows the aboslute levels, relative levels and data type"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def EvaluateMissingData(df):\n",
        "  missing_data_absolute = df.isnull().sum()\n",
        "  missing_data_percentage = round(missing_data_absolute/len(df)*100 , 2)\n",
        "  df_missing_data = (pd.DataFrame(\n",
        "                          data= {\"RowsWithMissingData\": missing_data_absolute,\n",
        "                                 \"PercentageOfDataset\": missing_data_percentage,\n",
        "                                 \"DataType\":df.dtypes}\n",
        "                                  )\n",
        "                    .sort_values(by=['PercentageOfDataset'],ascending=False)\n",
        "                    .query(\"PercentageOfDataset > 0\")\n",
        "                    )\n",
        "\n",
        "  return df_missing_data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Check missing data levels for collected dataset\n",
        "\n",
        "* Two variables, namely ['EnclosedPorch', 'WoodDeckSF'], have more than 85% missing values each."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "EvaluateMissingData(df)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uFQo3ycuO-v6"
      },
      "source": [
        "## Handling Missing Data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "* Defining DataCleaningEffect() function\n",
        "  * Code copied and adapted from Unit 9 of the feature engine lesson"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import seaborn as sns\n",
        "sns.set(style=\"whitegrid\")\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "def DataCleaningEffect(df_original,df_cleaned,variables_applied_with_method):\n",
        "\n",
        "  flag_count=1 # Indicate plot number\n",
        "  \n",
        "  # distinguish between numerical and categorical variables\n",
        "  categorical_variables = df_original.select_dtypes(exclude=['number']).columns \n",
        "\n",
        "  # scan over variables, \n",
        "    # first on variables that you applied the method\n",
        "    # if the variable is a numerical plot, a histogram if categorical plot a barplot\n",
        "  for set_of_variables in [variables_applied_with_method]:\n",
        "    print(\"\\n=====================================================================================\")\n",
        "    print(f\"* Distribution Effect Analysis After Data Cleaning Method in the following variables:\")\n",
        "    print(f\"{set_of_variables} \\n\\n\")\n",
        "  \n",
        "\n",
        "    for var in set_of_variables:\n",
        "      if var in categorical_variables:  # it is categorical variable: barplot\n",
        "        \n",
        "        df1 = pd.DataFrame({\"Type\":\"Original\",\"Value\":df_original[var]})\n",
        "        df2 = pd.DataFrame({\"Type\":\"Cleaned\",\"Value\":df_cleaned[var]})\n",
        "        dfAux = pd.concat([df1, df2], axis=0)\n",
        "        fig , axes = plt.subplots(figsize=(15, 5))\n",
        "        sns.countplot(hue='Type', data=dfAux, x=\"Value\",palette=['#432371',\"#FAAE7B\"])\n",
        "        axes.set(title=f\"Distribution Plot {flag_count}: {var}\")\n",
        "        plt.xticks(rotation=90)\n",
        "        plt.legend() \n",
        "\n",
        "      else: # it is numerical variable: histogram\n",
        "\n",
        "        fig , axes = plt.subplots(figsize=(10, 5))\n",
        "        sns.histplot(data=df_original, x=var, color=\"#432371\", label='Original', kde=True,element=\"step\", ax=axes)\n",
        "        sns.histplot(data=df_cleaned, x=var, color=\"#FAAE7B\", label='Cleaned', kde=True,element=\"step\", ax=axes)\n",
        "        axes.set(title=f\"Distribution Plot {flag_count}: {var}\")\n",
        "        plt.legend() \n",
        "\n",
        "      plt.show()\n",
        "      flag_count+= 1"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Data Cleaning Summary\n",
        "* Drop ['EnclosedPorch', 'WoodDeckSF'] because each has more than 80% missing values.\n",
        "* Imputation\n",
        "   * Mean- ['LotFrontage', 'BedroomAbvGr' ]\n",
        "   * Median- ['2ndFlrSF', 'GarageYrBlt', 'MasVnrArea' ]\n",
        "   * Categorical- ['GarageFinish','BsmtFinType1' ] "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Split Train and Test Set"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "TrainSet, TestSet, _, __ = train_test_split(\n",
        "                                        df,\n",
        "                                        df['SalePrice'],\n",
        "                                        test_size=0.2,\n",
        "                                        random_state=0)\n",
        "\n",
        "print(f\"TrainSet shape: {TrainSet.shape} \\nTestSet shape: {TestSet.shape}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Check the number of variables in the train set with missing values to make sure that the split represents the whole dataset."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "df_missing_data = EvaluateMissingData(TrainSet)\n",
        "print(f\"* There are {df_missing_data.shape[0]} variables with missing data \\n\")\n",
        "df_missing_data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 1. Drop variables\n",
        "* Variables to drop: ['EnclosedPorch', 'WoodDeckSF']\n",
        "* We create a separate data frame applying the imputation method and assess the effect of this data cleaning step.\n",
        "* List columns of the new data frame to see if the variables have beed dropped."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from feature_engine.selection import DropFeatures\n",
        "\n",
        "variables_to_drop=['EnclosedPorch' , 'WoodDeckSF']\n",
        "imputer = DropFeatures(features_to_drop=variables_to_drop)\n",
        "df_method = imputer.fit_transform(TrainSet)\n",
        "\n",
        "for i in variables_to_drop:\n",
        "    print(i in df_method.columns.to_list())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 2. Mean Imputation\n",
        "* Variables: ['LotFrontage' , 'BedroomAbvGr']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from feature_engine.imputation import MeanMedianImputer\n",
        "\n",
        "variables_mean = ['LotFrontage' , 'BedroomAbvGr']\n",
        "imputer = MeanMedianImputer(imputation_method='mean', variables=variables_mean)\n",
        "df_method = imputer.fit_transform(TrainSet)\n",
        "DataCleaningEffect(df_original=TrainSet,\n",
        "                   df_cleaned=df_method,\n",
        "                   variables_applied_with_method=variables_mean)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 3. Median Imputation\n",
        "* Variables: ['2ndFlrSF' , 'GarageYrBlt', 'MasVnrArea']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "variables_median = ['2ndFlrSF' , 'GarageYrBlt', 'MasVnrArea']\n",
        "imputer = MeanMedianImputer(imputation_method='median', variables=variables_median)\n",
        "df_method = imputer.fit_transform(TrainSet)\n",
        "DataCleaningEffect(df_original=TrainSet,\n",
        "                   df_cleaned=df_method,\n",
        "                   variables_applied_with_method=variables_median)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Looking at the effect of median imputation on GarageYrBlt, most of the imputed values are concentrated around 1975. \n",
        "We inspect if the missing values indicate that there is no garage at all.\n",
        "* The below data shows where GarageYrBlt is NaN, GarageArea is zero.\n",
        "* Intuitively, the size of the garage is likely to have more effect on sales price than its age. So we can consider dropping GarageYrBlt."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "TrainSet[(TrainSet['GarageArea'] ==0)][['GarageYrBlt', 'GarageArea']]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 4. Categorical Imputation\n",
        "* Variables: ['GarageFinish' , 'BsmtFinType1']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from feature_engine.imputation import CategoricalImputer\n",
        "\n",
        "variables_categorical = ['GarageFinish' , 'BsmtFinType1']\n",
        "imputer = CategoricalImputer(imputation_method='missing', fill_value='None', variables=variables_categorical)\n",
        "df_method = imputer.fit_transform(TrainSet)\n",
        "DataCleaningEffect(df_original=TrainSet,\n",
        "                   df_cleaned=df_method,\n",
        "                   variables_applied_with_method=variables_categorical)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "TrainSet[(TrainSet['GarageArea'] ==0)][['GarageFinish', 'GarageArea']]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "TrainSet[(TrainSet['TotalBsmtSF'] ==0)][['BsmtFinType1', 'TotalBsmtSF']]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Data Cleaning Pipeline"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from sklearn.pipeline import Pipeline\n",
        "\n",
        "dataCleaning_pipeline = Pipeline([\n",
        "      ( 'mean',  MeanMedianImputer(imputation_method='mean',\n",
        "                                     variables=['LotFrontage' , 'BedroomAbvGr']) ),\n",
        "      ( 'median',  MeanMedianImputer(imputation_method='median',\n",
        "                                     variables=['2ndFlrSF', 'MasVnrArea']) ),\n",
        "      ( 'categorical',  CategoricalImputer(imputation_method='missing',\n",
        "                                     fill_value='None',\n",
        "                                     variables=['GarageFinish' , 'BsmtFinType1']) ),\n",
        "      ( 'drop',  DropFeatures(features_to_drop=['EnclosedPorch', 'GarageYrBlt', 'WoodDeckSF']) )\n",
        "])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Apply the pipeline to the whole dataset to get cleaned data."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "TrainSet, TestSet = dataCleaning_pipeline.fit_transform(TrainSet) , dataCleaning_pipeline.fit_transform(TestSet)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "df = dataCleaning_pipeline.fit_transform(df)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "EvaluateMissingData(TrainSet)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "EvaluateMissingData(TestSet)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "EvaluateMissingData(df)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ltNetd085qHf"
      },
      "source": [
        "## Push files to Repo"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aKlnIozA4eQO",
        "outputId": "fd09bc1f-adb1-4511-f6ce-492a6af570c0"
      },
      "outputs": [],
      "source": [
        "try:\n",
        "  os.makedirs(name='outputs/datasets/cleaned')\n",
        "except Exception as e:\n",
        "  print(e)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Train Set"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "TrainSet.to_csv(\"outputs/datasets/cleaned/TrainSetCleaned.csv\", index=False)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Test Set"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "TestSet.to_csv(\"outputs/datasets/cleaned/TestSetCleaned.csv\", index=False)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Cleaned Dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "df.to_csv(\"outputs/datasets/cleaned/HousePricesCleaned.csv\", index=False)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Save the data cleaning pipeline for later use to clean inheritted houses data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import joblib\n",
        "\n",
        "file_path = f'outputs/ml_pipeline/data_cleaning'\n",
        "\n",
        "try:\n",
        "  os.makedirs(name=file_path)\n",
        "except Exception as e:\n",
        "  print(e)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "joblib.dump(value=dataClean_pipeline, filename=f\"{file_path}/dataCleaning_pipeline.pkl\")"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "Data Practitioner Jupyter Notebook.ipynb",
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3.8.12 64-bit ('3.8.12')",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.12"
    },
    "orig_nbformat": 2,
    "vscode": {
      "interpreter": {
        "hash": "8b8334dab9339717f727a1deaf837b322d7a41c20d15cc86be99a8e69ceec8ce"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
